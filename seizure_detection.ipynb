{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install necessary packages\n",
    "\n",
    "%pip install matplotlib pandas numpy scipy seaborn mne\n",
    "%pip install beautifulsoup4 requests wget\n",
    "%pip install h5py tables kaggle\n",
    "%pip install wfdb pyEDFlib PyWavelets\n",
    "%pip install keras\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Import data into panda data frame\n",
    "data = pd.read_csv(r'C:\\Users\\shortallb\\Documents\\Research\\Epileptic Seizure Data - Kaggle\\data.csv')\n",
    "n = data.shape[0]\n",
    "#Remove patient name \n",
    "data = data.drop(data.columns[0], axis =1)\n",
    "#Extract Labels\n",
    "labels = data.iloc[:,178].to_numpy()\n",
    "data = data.drop(data.columns[178], axis =1)\n",
    "\n",
    "#Reduce num classes from 5 to 2. (binary classifier)\n",
    "labels_bin = np.zeros(n)\n",
    "for i in range(n):\n",
    "    if labels[i] == 1:\n",
    "        labels_bin[i] = 1\n",
    "       \n",
    "#Import \"Real\" data from .CSV \n",
    "b1 = pd.read_csv('./baseline.csv', header=None).to_numpy()\n",
    "s1 = pd.read_csv('./seizure1.csv', header=None).to_numpy()\n",
    "s2 = pd.read_csv('./seizure2.csv', header=None).to_numpy()\n",
    "\n",
    "#Convert to numpy array \n",
    "#data = data.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to compute mean band powers\n",
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "sample_rate = 178 # in hz\n",
    "\n",
    "data_test1 = data.iloc[0]    #Non-Seizure data\n",
    "data_test2 = data.iloc[1]   #Seizure data\n",
    "\n",
    "#Function gets the mean power of a specified frequency band. Will be used to calculate power estimations of most common frequency bands\n",
    "def bandpower(data, sf, band, output = False):\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "\n",
    "    # Compute the periodogram (Welch)\n",
    "    freqs, psd = welch(data, \n",
    "                       sf, \n",
    "                       nperseg=(2 / low)*sf,\n",
    "                       scaling='density', \n",
    "                       axis=0)\n",
    "    \n",
    "    # put into a df\n",
    "    psd = pd.DataFrame(psd, index = freqs)\n",
    "    \n",
    "    if output:\n",
    "        print('Welch Output')\n",
    "        psd.index.name = 'Hz'\n",
    "        psd.columns = ['Power']\n",
    "        display(psd)\n",
    "    \n",
    "    # Find closest indices of band in frequency vector\n",
    "    idx_min = np.argmax(np.round(freqs) > low) - 1\n",
    "    idx_max = np.argmax(np.round(freqs) > high)\n",
    "    \n",
    "    # select frequencies of interest\n",
    "    psd = psd.iloc[idx_min:idx_max,:]\n",
    "    \n",
    "    # get the mean of each channel over all frequencies in the band\n",
    "    psd = psd.mean()\n",
    "    \n",
    "    if output:\n",
    "        print('\\nMean Frequency Band')\n",
    "        display(psd)\n",
    "    \n",
    "    return psd\n",
    "\n",
    "#Returns df of power densities for frequency bands\n",
    "def power_measures(data, sample_rate, output=False):\n",
    "    bandpasses = [[[0.1,4],'power_delta'],\n",
    "                  [[4,8],'power_theta'],\n",
    "                  [[8,12],'power_alpha'],\n",
    "                  [[12,30],'power_beta'],\n",
    "                  [[30,70],'power_gamma']\n",
    "                 ]\n",
    "    \n",
    "    welch_df = pd.DataFrame()\n",
    "    for bandpass, freq_name in bandpasses:\n",
    "        bandpass_data = bandpower(data, sample_rate, bandpass)\n",
    "        bandpass_data.index = [freq_name]\n",
    "        \n",
    "        if welch_df.empty:\n",
    "            welch_df = bandpass_data\n",
    "\n",
    "        else:\n",
    "            welch_df = pd.concat([welch_df, bandpass_data])\n",
    "        \n",
    "    welch_df = welch_df.T\n",
    "    \n",
    "    if output:\n",
    "        display(welch_df)\n",
    "    \n",
    "    return welch_df\n",
    "#Code found from:     \n",
    "#https://colab.research.google.com/github/Eldave93/Seizure-Detection-Tutorials/blob/master/02.%20Pre-Processing%20%26%20Feature%20Engineering.ipynb#scrollTo=Oe3h7w3tVbhK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shortallb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\scipy\\signal\\spectral.py:1969: UserWarning: nperseg = 3560 is greater than input length  = 178, using nperseg = 178\n",
      "  .format(nperseg, input_length))\n"
     ]
    }
   ],
   "source": [
    "#Convert signal data into power densities Col 0 - 5 : delta, theta, alpha, beta, gamma\n",
    "power_data = np.empty((n,5))\n",
    "sample_rate = 178 #Hz\n",
    "for i in range(n):\n",
    "    power_data[i] = power_measures(data.iloc[i], sample_rate, output=False).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39266835 0.10220203 0.2879403  0.17819973 0.03898958]\n",
      "1539.9670415388803\n",
      "[604.69632186 157.38776484 443.41857731 274.42171134  60.04266618]\n"
     ]
    }
   ],
   "source": [
    "data_test1 = data.iloc[0]\n",
    "power_data = power_measures(data_test1, sample_rate, output=False).to_numpy()\n",
    "power_sum = np.sum(power_data)\n",
    "band_power_norm = power_data / power_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into data and labels\n",
    "x = np.concatenate((b1,s1,s2), axis = 0)\n",
    "y = x[:,10000]\n",
    "x = np.delete(x, 10000, axis=1)\n",
    "#Shuffle and partion data into train and test. (20% total data used as test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=.2, random_state=27)\n",
    "\n",
    "#Reshape for LSTM input\n",
    "X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ai-leader.com/2020/05/09/using-lstm-with-1d-2d-and-3d-array/\n",
    "from keras import Model\n",
    "from keras.layers import Input, Dense, Bidirectional\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "#construct callbacaks\n",
    "#backup model following each epoch \n",
    "backup_callback = tf.keras.callbacks.BackupAndRestore(backup_dir=\"./tmp/backup\")\n",
    "#stop after accuracy becomes stagnant\n",
    "stop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"accuracy\",\n",
    "    min_delta=.0011,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    baseline=None,\n",
    "    restore_best_weights=False,\n",
    "    start_from_epoch=0,\n",
    ")\n",
    "#CSV logger - outputs progress to csv file\n",
    "csv_callback = tf.keras.callbacks.CSVLogger('./train_progress.csv', separator=\",\", append=True)\n",
    "\n",
    "def define_module():\n",
    "    #shape is hardcoded. change if need be\n",
    "    input1= Input(shape=(10000, 1))\n",
    "    lstm1 = Bidirectional(LSTM(units=32))(input1)\n",
    "    dnn_hidden_layer1 = Dense(3, activation='relu')(lstm1)\n",
    "    dnn_output = Dense(1, activation='sigmoid')(dnn_hidden_layer1)\n",
    "    model = Model(inputs=[input1],outputs=[dnn_output])\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://ieeexplore.ieee.org/document/9873913\n",
    "\n",
    "#https://www.frontiersin.org/articles/10.3389/fncom.2021.650050/full\n",
    "#Zscore normalization if we want to use EEG data and electrode data\n",
    "\n",
    "#https://datascience.stackexchange.com/questions/33393/understanding-input-of-lstm\n",
    "#how to format LSTM input\n",
    "\n",
    "#https://keras.io/api/callbacks/tensorboard/\n",
    "#callback functions\n",
    "model.save('./LSTM1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run a single model using random state shuffle \n",
    "lstm2 = define_module()\n",
    "lstm2.fit(X_train, Y_train, epochs=10, batch_size = X_train.shape[0], callbacks = [backup_callback, stop_callback, csv_callback])\n",
    "lstm2.save('./LSTM2.hdf5')\n",
    "scores = lstm2.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-5e69d7769d50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#concatenate train and test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "#kfold cross validation source: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-k-fold-cross-validation-with-keras.md\n",
    "from sklearn.model_selection import KFold\n",
    "#concatenate train and test data\n",
    "inputs = np.concatenate(X_train, X_test)\n",
    "labels = np.concatenate(Y_train, Y_test)\n",
    "\n",
    "#define num folds and cross validator\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "accuracies = []\n",
    "losses = []\n",
    "\n",
    "# Construct and Evaluate models\n",
    "fold_num = 1\n",
    "for train, test in kfold.split(inputs, labels):\n",
    "    model = define_module()\n",
    "    #fit data\n",
    "    history = model.fit(inputs[train], labels[train], batch_size = X_train.shape[0], epochs=10, callbacks = [backup_callback, stop_callback, csv_callback])\n",
    "    #evaluate test data\n",
    "    scores = model.evaluate(inputs[test], labels[test])\n",
    "    #append accuracies/losses to array of all folds\n",
    "    accuracies.append(scores[1])\n",
    "    losses.append(scores[0])\n",
    "    #increment fold num\n",
    "    fold_num += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85c3d2ce18a6859a1d2dde2b94a121eb64f0b520c961cfccf4ca2c96219660ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
